{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c2bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 13:05:27.301006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744311927.315296   29083 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744311927.319892   29083 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-10 13:05:27.336062: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3096 files belonging to 4 classes.\n",
      "Using 2477 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744311929.779324   29083 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3096 files belonging to 4 classes.\n",
      "Using 619 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Path to the main folder containing subfolders for each class\n",
    "data_dir = \"../Data\"\n",
    "\n",
    "# Create training and validation datasets\n",
    "batch_size = 32\n",
    "img_size = (256, 256)\n",
    "seed = 123\n",
    "\n",
    "# Load the data and split it into training and validation sets\n",
    "train_ds = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",  # This specifies the subset to be used for training\n",
    "    seed=seed,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\"  # Important: images are in grayscale, so specify this mode\n",
    ")\n",
    "\n",
    "val_ds = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",  # This specifies the subset to be used for validation\n",
    "    seed=seed,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"grayscale\"  # Same as for training, images are in grayscale\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac04dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4237f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/projects/tf217/env/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Build the model using a sequential approach\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(256, 256, 1)),  # Normalize pixel values to [0, 1]\n",
    "\n",
    "    # Convolutional Layer 1: 64 filters, 3x3 kernel, ReLU activation, and same padding\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(),  # MaxPooling to reduce spatial dimensions\n",
    "    layers.Dropout(0.4),  # Dropout to prevent overfitting after the first convolutional layer\n",
    "\n",
    "    # Convolutional Layer 2: 128 filters, 3x3 kernel, ReLU activation, and same padding\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.4),  # Dropout after the second convolutional layer\n",
    "\n",
    "    # Convolutional Layer 3: 256 filters, 3x3 kernel, ReLU activation, and same padding\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.4),  # Dropout after the third convolutional layer\n",
    "\n",
    "    layers.Flatten(),  # Flatten the output to feed into the fully connected layer\n",
    "    layers.Dense(256, activation='relu'),  # Dense layer with 256 units and ReLU activation\n",
    "    layers.Dropout(0.5),  # Dropout on the fully connected layer to reduce overfitting\n",
    "    layers.Dense(4, activation='softmax')  # Output layer for multi-class classification (4 classes)\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # Suitable for multi-class classification with integer labels\n",
    "    metrics=['accuracy']  # Track accuracy during training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744311933.714434   29166 service.cc:148] XLA service 0x7f25d0004ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744311933.714503   29166 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3070, Compute Capability 8.6\n",
      "2025-04-10 13:05:33.745669: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1744311933.881162   29166 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1744311942.388247   29166 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 167ms/step - accuracy: 0.3237 - loss: 2.1482 - val_accuracy: 0.4701 - val_loss: 1.2742 - learning_rate: 0.0010\n",
      "Epoch 2/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.5131 - loss: 1.1790 - val_accuracy: 0.5897 - val_loss: 1.0273 - learning_rate: 0.0010\n",
      "Epoch 3/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.5834 - loss: 0.9881 - val_accuracy: 0.6947 - val_loss: 0.8716 - learning_rate: 0.0010\n",
      "Epoch 4/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - accuracy: 0.6643 - loss: 0.8181 - val_accuracy: 0.7383 - val_loss: 0.7345 - learning_rate: 0.0010\n",
      "Epoch 5/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.7190 - loss: 0.7168 - val_accuracy: 0.7577 - val_loss: 0.6793 - learning_rate: 0.0010\n",
      "Epoch 6/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.7689 - loss: 0.6075 - val_accuracy: 0.7803 - val_loss: 0.6600 - learning_rate: 0.0010\n",
      "Epoch 7/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.7967 - loss: 0.5101 - val_accuracy: 0.7997 - val_loss: 0.5836 - learning_rate: 0.0010\n",
      "Epoch 8/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - accuracy: 0.8321 - loss: 0.4313 - val_accuracy: 0.8336 - val_loss: 0.5238 - learning_rate: 0.0010\n",
      "Epoch 9/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.8691 - loss: 0.3588 - val_accuracy: 0.8384 - val_loss: 0.5234 - learning_rate: 0.0010\n",
      "Epoch 10/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - accuracy: 0.8861 - loss: 0.2913 - val_accuracy: 0.8578 - val_loss: 0.4986 - learning_rate: 0.0010\n",
      "Epoch 11/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - accuracy: 0.8883 - loss: 0.2803 - val_accuracy: 0.8675 - val_loss: 0.4943 - learning_rate: 0.0010\n",
      "Epoch 12/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9152 - loss: 0.2209 - val_accuracy: 0.8643 - val_loss: 0.5728 - learning_rate: 0.0010\n",
      "Epoch 13/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 0.9226 - loss: 0.2148 - val_accuracy: 0.8821 - val_loss: 0.4902 - learning_rate: 0.0010\n",
      "Epoch 14/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - accuracy: 0.9263 - loss: 0.1926 - val_accuracy: 0.8611 - val_loss: 0.5116 - learning_rate: 0.0010\n",
      "Epoch 15/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - accuracy: 0.9418 - loss: 0.1705 - val_accuracy: 0.8417 - val_loss: 0.6065 - learning_rate: 0.0010\n",
      "Epoch 16/59\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9541 - loss: 0.1465\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - accuracy: 0.9540 - loss: 0.1465 - val_accuracy: 0.8837 - val_loss: 0.5832 - learning_rate: 0.0010\n",
      "Epoch 17/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9637 - loss: 0.1250 - val_accuracy: 0.8837 - val_loss: 0.5899 - learning_rate: 5.0000e-04\n",
      "Epoch 18/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9606 - loss: 0.1108 - val_accuracy: 0.8918 - val_loss: 0.5902 - learning_rate: 5.0000e-04\n",
      "Epoch 19/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9722 - loss: 0.0933\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9722 - loss: 0.0932 - val_accuracy: 0.8966 - val_loss: 0.6176 - learning_rate: 5.0000e-04\n",
      "Epoch 20/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9722 - loss: 0.0769 - val_accuracy: 0.8950 - val_loss: 0.6053 - learning_rate: 2.5000e-04\n",
      "Epoch 21/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9801 - loss: 0.0573 - val_accuracy: 0.8934 - val_loss: 0.6452 - learning_rate: 2.5000e-04\n",
      "Epoch 22/59\n",
      "\u001b[1m76/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9656 - loss: 0.0832\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - accuracy: 0.9659 - loss: 0.0828 - val_accuracy: 0.8950 - val_loss: 0.6396 - learning_rate: 2.5000e-04\n",
      "Epoch 23/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.9747 - loss: 0.0689 - val_accuracy: 0.8966 - val_loss: 0.6519 - learning_rate: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True  # Stop training if validation loss doesn't improve for 10 epochs, restore best weights\n",
    ")\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6  # Reduce learning rate by a factor of 0.5 if validation loss plateaus for 3 epochs\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=59,  # Train for 59 epochs\n",
    "    callbacks=[early_stop, lr_scheduler]  # Use early stopping and learning rate scheduler callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f93d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/projects/tf217/env/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the pre-trained Xception model without the final layers\n",
    "base_model = Xception(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "# Freeze the Xception layers to prevent them from being updated during training\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the model by stacking layers\n",
    "model_xception = models.Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(256, 256, 1)),  # Normalize pixel values to [0, 1]\n",
    "    layers.Conv2D(3, (3, 3), padding='same', activation='relu'),  # Convert grayscale images to 3 channels for Xception input\n",
    "    base_model,  # Use the pre-trained Xception model as a base\n",
    "    layers.GlobalAveragePooling2D(),  # Global average pooling to reduce spatial dimensions\n",
    "    layers.Dense(256, activation='relu'),  # Dense layer with 256 units and ReLU activation\n",
    "    layers.Dropout(0.5),  # Dropout to prevent overfitting\n",
    "    layers.Dense(4, activation='softmax')  # Output layer for multi-class classification (4 classes)\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
    "model_xception.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb016b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 283ms/step - accuracy: 0.4874 - loss: 1.1352 - val_accuracy: 0.7528 - val_loss: 0.6917 - learning_rate: 0.0010\n",
      "Epoch 2/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.7327 - loss: 0.6741 - val_accuracy: 0.7932 - val_loss: 0.5757 - learning_rate: 0.0010\n",
      "Epoch 3/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.7812 - loss: 0.5714 - val_accuracy: 0.7803 - val_loss: 0.5680 - learning_rate: 0.0010\n",
      "Epoch 4/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - accuracy: 0.7900 - loss: 0.5303 - val_accuracy: 0.7997 - val_loss: 0.5327 - learning_rate: 0.0010\n",
      "Epoch 5/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.8322 - loss: 0.4812 - val_accuracy: 0.8045 - val_loss: 0.5136 - learning_rate: 0.0010\n",
      "Epoch 6/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 120ms/step - accuracy: 0.8513 - loss: 0.4103 - val_accuracy: 0.8174 - val_loss: 0.5216 - learning_rate: 0.0010\n",
      "Epoch 7/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.8548 - loss: 0.4082 - val_accuracy: 0.8223 - val_loss: 0.4759 - learning_rate: 0.0010\n",
      "Epoch 8/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 98ms/step - accuracy: 0.8763 - loss: 0.3273 - val_accuracy: 0.8417 - val_loss: 0.4661 - learning_rate: 0.0010\n",
      "Epoch 9/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.8697 - loss: 0.3283 - val_accuracy: 0.8546 - val_loss: 0.4338 - learning_rate: 0.0010\n",
      "Epoch 10/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.8998 - loss: 0.2873 - val_accuracy: 0.8401 - val_loss: 0.4502 - learning_rate: 0.0010\n",
      "Epoch 11/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 98ms/step - accuracy: 0.8879 - loss: 0.2741 - val_accuracy: 0.8578 - val_loss: 0.4306 - learning_rate: 0.0010\n",
      "Epoch 12/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 149ms/step - accuracy: 0.9084 - loss: 0.2561 - val_accuracy: 0.8384 - val_loss: 0.4267 - learning_rate: 0.0010\n",
      "Epoch 13/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.8956 - loss: 0.2553 - val_accuracy: 0.8288 - val_loss: 0.4513 - learning_rate: 0.0010\n",
      "Epoch 14/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 97ms/step - accuracy: 0.9126 - loss: 0.2271 - val_accuracy: 0.8595 - val_loss: 0.4281 - learning_rate: 0.0010\n",
      "Epoch 15/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9268 - loss: 0.2007\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.9267 - loss: 0.2009 - val_accuracy: 0.8498 - val_loss: 0.4571 - learning_rate: 0.0010\n",
      "Epoch 16/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.9255 - loss: 0.1959 - val_accuracy: 0.8691 - val_loss: 0.4256 - learning_rate: 5.0000e-04\n",
      "Epoch 17/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.9541 - loss: 0.1530 - val_accuracy: 0.8562 - val_loss: 0.4429 - learning_rate: 5.0000e-04\n",
      "Epoch 18/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - accuracy: 0.9538 - loss: 0.1314 - val_accuracy: 0.8659 - val_loss: 0.4401 - learning_rate: 5.0000e-04\n",
      "Epoch 19/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9584 - loss: 0.1234\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.9584 - loss: 0.1234 - val_accuracy: 0.8578 - val_loss: 0.4532 - learning_rate: 5.0000e-04\n",
      "Epoch 20/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.9711 - loss: 0.1085 - val_accuracy: 0.8611 - val_loss: 0.4400 - learning_rate: 2.5000e-04\n",
      "Epoch 21/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - accuracy: 0.9743 - loss: 0.0996 - val_accuracy: 0.8708 - val_loss: 0.4399 - learning_rate: 2.5000e-04\n",
      "Epoch 22/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9697 - loss: 0.1087\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.9697 - loss: 0.1086 - val_accuracy: 0.8708 - val_loss: 0.4372 - learning_rate: 2.5000e-04\n",
      "Epoch 23/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.9681 - loss: 0.0993 - val_accuracy: 0.8724 - val_loss: 0.4399 - learning_rate: 1.2500e-04\n",
      "Epoch 24/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 119ms/step - accuracy: 0.9731 - loss: 0.0883 - val_accuracy: 0.8675 - val_loss: 0.4439 - learning_rate: 1.2500e-04\n",
      "Epoch 25/59\n",
      "\u001b[1m77/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9675 - loss: 0.0945\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.9676 - loss: 0.0944 - val_accuracy: 0.8691 - val_loss: 0.4424 - learning_rate: 1.2500e-04\n",
      "Epoch 26/59\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.9708 - loss: 0.0971 - val_accuracy: 0.8724 - val_loss: 0.4404 - learning_rate: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "# Training the model_2 (Xception + custom layers)\n",
    "history_xception = model_xception.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=59,  # Number of epochs for training\n",
    "    callbacks=[early_stop, lr_scheduler],  # Callbacks for EarlyStopping and ReduceLROnPlateau\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "492ed361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8833 - loss: 0.4616\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8728 - loss: 0.4042\n",
      "Original Model - Loss: 0.4902, Accuracy: 0.8821\n",
      "Xception Model - Loss: 0.4256, Accuracy: 0.8691\n"
     ]
    }
   ],
   "source": [
    "# Evaluate both models on the validation dataset\n",
    "loss_original, accuracy_original = model.evaluate(val_ds)\n",
    "loss_xception, accuracy_xception = model_xception.evaluate(val_ds)\n",
    "\n",
    "# Print the comparison of the two models\n",
    "print(f\"Original Model - Loss: {loss_original:.4f}, Accuracy: {accuracy_original:.4f}\")\n",
    "print(f\"Xception Model - Loss: {loss_xception:.4f}, Accuracy: {accuracy_xception:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
